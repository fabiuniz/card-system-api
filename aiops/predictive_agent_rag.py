import requests
import time
import os
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# ConfiguraÃ§Ãµes
OLLAMA_URL = "http://ollama-server:11434/api/generate"
PROMETHEUS_URL = "http://prometheus:9090/api/v1/query"

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

def get_context(query):
    if os.path.exists("./vector_db"):
        vector_db = Chroma(persist_directory="./vector_db", embedding_function=embeddings)
        results = vector_db.similarity_search(query, k=2)
        return "\n".join([res.page_content for res in results])
    return "Nenhum conhecimento prÃ©vio encontrado."

def ask_ollama(metrics, context):
    prompt = f"""
    CONTEXTO TÃ‰CNICO (InstruÃ§Ãµes do Fabiano):
    {context}

    MÃ‰TRICAS ATUAIS:
    {metrics}

    Como Engenheiro SRE, analise se hÃ¡ tendÃªncia de falha e sugira a correÃ§Ã£o baseada no contexto.
    """
    payload = {"model": "llama3", "prompt": prompt, "stream": False}
    try:
        res = requests.post(OLLAMA_URL, json=payload)
        return res.json()['response']
    except:
        return "Aguardando inicializaÃ§Ã£o do Ollama..."

print("ðŸš€ Agente Preditivo Rodando...")
while True:
    # SimulaÃ§Ã£o de busca de mÃ©trica (aqui conectaria no Prometheus real)
    mock_metrics = "LatÃªncia: 250ms, Erros 5xx: 2%"
    context = get_context("latÃªncia alta e erros de servidor")
    insight = ask_ollama(mock_metrics, context)
    print(f"\nðŸ§  [IA Insight]: {insight}")
    time.sleep(60)

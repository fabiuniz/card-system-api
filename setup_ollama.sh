#!/bin/bash

echo "ü§ñ [SRE C√≥rtex] Iniciando instala√ß√£o da IA Preditiva Santander..."

# 1. CRIANDO ESTRUTURA DE DIRET√ìRIOS


# 2. GERANDO O AGENTE PREDITIVO (Python + LangChain + RAG)
cat <<'EOF' > aiops/predictive_agent_rag.py
import requests
import time
import os
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# Configura√ß√µes
OLLAMA_URL = "http://ollama-server:11434/api/generate"
PROMETHEUS_URL = "http://prometheus:9090/api/v1/query"

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

def get_context(query):
    if os.path.exists("./vector_db"):
        vector_db = Chroma(persist_directory="./vector_db", embedding_function=embeddings)
        results = vector_db.similarity_search(query, k=2)
        return "\n".join([res.page_content for res in results])
    return "Nenhum conhecimento pr√©vio encontrado."

def ask_ollama(metrics, context):
    prompt = f"""
    CONTEXTO T√âCNICO (Instru√ß√µes do Fabiano):
    {context}

    M√âTRICAS ATUAIS:
    {metrics}

    Como Engenheiro SRE, analise se h√° tend√™ncia de falha e sugira a corre√ß√£o baseada no contexto.
    """
    payload = {"model": "llama3", "prompt": prompt, "stream": False}
    try:
        res = requests.post(OLLAMA_URL, json=payload)
        return res.json()['response']
    except:
        return "Aguardando inicializa√ß√£o do Ollama..."

print("üöÄ Agente Preditivo Rodando...")
while True:
    # Simula√ß√£o de busca de m√©trica (aqui conectaria no Prometheus real)
    mock_metrics = "Lat√™ncia: 250ms, Erros 5xx: 2%"
    context = get_context("lat√™ncia alta e erros de servidor")
    insight = ask_ollama(mock_metrics, context)
    print(f"\nüß† [IA Insight]: {insight}")
    time.sleep(60)
EOF

# 3. GERANDO O SCRIPT DE RE-INDEXA√á√ÉO (Afinamento)
cat <<'EOF' > aiops/reindex_brain.py
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import DirectoryLoader, TextLoader

print("üîÑ Sincronizando novos conhecimentos...")
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
loader = DirectoryLoader('./brain', glob="**/*.md", loader_cls=TextLoader)
documents = loader.load()

if documents:
    vector_db = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory="./vector_db")
    print(f"‚úÖ {len(documents)} arquivos de conhecimento indexados.")
else:
    print("‚ö†Ô∏è Pasta 'brain' vazia. Adicione arquivos .md para ensinar a IA.")
EOF

# 4. GERANDO O DOCKERFILE DO AGENTE
cat <<EOF > aiops/Dockerfile.ai
FROM python:3.9-slim
WORKDIR /app
RUN pip install requests prometheus-api-client langchain langchain-community chromadb sentence-transformers
COPY . .
CMD ["python", "predictive_agent_rag.py"]
EOF

# 5. GERANDO O UTILIT√ÅRIO add_knowledge.sh
cat <<'EOF' > add_knowledge.sh
#!/bin/bash
if [ -z "$1" ]; then
    echo "Uso: ./add_knowledge.sh 'Minha instru√ß√£o para a IA'"
    exit 1
fi
echo "$1" > aiops/brain/memo_$(date +%s).md
docker exec -it ai-agent python3 reindex_brain.py
echo "‚úÖ IA atualizada!"
EOF
chmod +x add_knowledge.sh

echo "--------------------------------------------------------"
echo "‚úÖ TUDO PRONTO! O C√©rebro RAG foi configurado."
echo "1. Execute 'docker-compose up -d' para subir a IA."
echo "2. Baixe o modelo: 'docker exec -it ollama-server ollama run llama3'"
echo "3. Use './add_knowledge.sh' para afinar o agente em tempo real."
echo "--------------------------------------------------------"


#M√°quina A (A "Poderosa" com RX 580):
#Ollama + Llama 3 (RAG): Essa GPU AMD aguenta o modelo de 8 bilh√µes de par√¢metros.
#Banco de Vetores (ChromaDB): Onde fica o conhecimento.
#Grafana/Prometheus: O centro de controle.
#M√°quina B (A "Est√°vel" com GTX 760):
#API Java (Spring Boot): Rodando o core business.
#Os 3 Bancos de Dados (MySQL, Postgres, Mongo): Usando os 16GB de RAM para cache.
#Os 3 Front-ends: Servindo as interfaces.